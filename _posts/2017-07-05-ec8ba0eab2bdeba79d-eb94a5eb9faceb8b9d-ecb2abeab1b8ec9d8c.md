---
id: 600
title: 신경망, 딥러닝 첫걸음
date: 2017-07-05T09:37:30+09:00
author: philoz
layout: post
guid: http://localhost/wordpress/?p=600
permalink: '/2017/07/05/%ec%8b%a0%ea%b2%bd%eb%a7%9d-%eb%94%a5%eb%9f%ac%eb%8b%9d-%ec%b2%ab%ea%b1%b8%ec%9d%8c/'
image: /wp-content/uploads/2017/07/-2017-07-06-오전-1.48.11.png
tags: [develop]
categories: [develop, neural network]
---
요즘 딥러닝 오픈소스도 잘 개발되어 있다고 해서 API를 갖다쓰려면 공부를 좀 해봐야겠다고 마음 먹고 책을 읽었다.

책은 머신러닝 딥러닝 신경망에 대해 간단한 수준의 개념 설명과 손쉬운 구현이 가능한 코드로 구성되어있다. &nbsp; &nbsp;최신기술인 만큼 더 어렵고 복잡한 수식이 들어갈 것이라 생각했는데 생각보다 이해하기 어렵지 않았다.

구체적인 수식 도출을 제외하고 개념정도만 이해해도 소스 사용에는 문제가 없을 것으로 보여서 정리해둔다.
<blockquote>1. 인공지능, 머신러닝, 딥러닝, 신경망의 개념</blockquote>
[caption id="attachment_620" align="alignnone" width="445"]<img class="size-full wp-image-620" src="/assets/wp-content/uploads/2017/07/-2017-07-06-오전-1.48.11.png" alt="diagram1" width="445" height="465"> 인공지능, 머신러닝, 딥러닝의 관계[/caption]

<!--more-->

인공지능: 인간과 비슷하게 사고하는 컴퓨터 지능
머신러닝: 데이터를 통해 컴퓨터를 학습시켜서 인공지능의 성능을 향상시키는 기법
딥러닝: 신경망을 기반으로 한 머신러닝의 기법 중 하나
<blockquote>2. 신경망 작동원리</blockquote>
[caption id="attachment_627" align="aligncenter" width="769"]<img class="size-full wp-image-627" src="/assets/wp-content/uploads/2017/07/-2017-07-06-오전-2.18.23.png" alt="How neural network works" width="769" height="686"> 다층 신경망 작동원리[/caption]

신경망은 은닉층 여부에 따라 단층, 다층 신경망으로 나뉜다.
현재 일반적으로 사용되는 다층 신경망은 은닉층의 가중치(그림에서 각 노드를 잇는 선으로 표현)를 적절히 안배하여 원하는 결과를 얻는다.
이 가중치를 조절하는 것을 "학습"시킨다라고 하며 인간이 아니라 노드와 가중치의 관계만 설정해주면 나머지는 컴퓨터가 알고리즘을 통해 가중치를 조절하는 식이다.

이 가중치의 조절 방식이 신경망의 핵심이다. 모든 가중치는 초기에 랜덤값으로 설정해도 무방하며, 오차의 역전파 알고리즘에 따라 수정된다. 여기서 오차란 각 노드의 출력단계와 해당 노드의 차이를 말하는 것으로, 단순 뺄셈, 차이의 제곱 등 오차의 계산방식은 "비용함수"라고 한다. 계산된 오차를 각 가중치의 비율에 따라 다시 노드에 전달하고, 노드는 전달받은 오차에 따라 가중치를 변경한다. 이 때 오차가 출력층-&gt;입력층으로 이동하며 가중치를 변경시키기 때문에 'back' propagation(거꾸로 전파됨-&gt;역전파) 이라고 부르는 것이다.

이렇게 입력노드에까지 도달하면 학습이 완료된다.
<blockquote>3. 딥러닝이란?</blockquote>
딥러닝은 단순하게는 다층 신경망을 의미한다. 이것은 그전까지 단층 신경망(은닉층이 없음)으로 "선형 문제"만 해결 가능했던 것이, 은닉층의 추가로 "비선형 문제"까지 해결가능하게 되면서 정의내린 것이다.

단순히 은닉층(몇 개 층이던 상관없다) 하나 추가가 그렇게 어려웠던 것일까? 책에서는 무려 30년 동안 이것을 해결 못해서 딥러닝 연구가 침체에 빠졌다고 설명한다.

다시 신경망의 설명을 돌아보자면, 오차의 역전파를 위해서 먼저 오차를 구하고 각 가중치에 따라 그 전 노드에 전달해준다고 했다. 이 때 출력층의 오차는 당연히 (출력값-정답)이겠지만, 은닉층에서의 오차는 무엇일까? 이 값은 우리가 정의내릴 수 없기 때문에 신경망 연구가 한 때 좌절되었던 것이다.

사실 지금까지 역전파라고 불렀지만, 역전파 알고리즘은 은닉층에서 그 전 노드의 오차를 이용해 다음 오차를 구하는 알고리즘을 말하며 이것이전에는 역전파가 아니라 단순 계산이었던 것 같다. 어쨌든 역전파 알고리즘이 지금 말한듯이 개발되면서 문제가 해결됐다.(현재 노드 오차 = 가중치의 Transpose 행렬 * 그 전 노드오차 행렬)

*이상 신경망 첫걸음(2017, 타리크 라시드) / 딥러닝 첫걸음(2016, 김성필)을 읽고 정리함